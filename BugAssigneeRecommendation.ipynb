{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QOvn66pmjoPJ","executionInfo":{"status":"ok","timestamp":1642516088950,"user_tz":-60,"elapsed":25017,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"a4b7fe7c-6be5-4064-d6da-1cb4b5df18f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.8 MB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 42.5 MB/s \n","\u001b[K     |████████████████████████████████| 213 kB 50.3 MB/s \n","\u001b[K     |████████████████████████████████| 90 kB 9.1 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n","\u001b[K     |████████████████████████████████| 47.7 MB 2.5 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 48.4 MB/s \n","\u001b[K     |████████████████████████████████| 352 kB 54.3 MB/s \n","\u001b[K     |████████████████████████████████| 99 kB 10.2 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 53.4 MB/s \n","\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["import os\n","# to use or not to use GPU\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n","\n","# packages for model graph visualization\n","#!pip install -q pydot\n","# install graphviz https://graphviz.gitlab.io/download/ \n","\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import tensorflow_hub as hub\n","!pip install -q -U tensorflow-text\n","import tensorflow_text as text\n","!pip install -q tf-models-official\n","from official.nlp import optimization  # to create AdamW optimizer\n","\n","import json\n","import re\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#save_path = \"/content/drive/My Drive/Colab Notebooks/\""]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-eD1puUmjrzq","executionInfo":{"status":"ok","timestamp":1642516343842,"user_tz":-60,"elapsed":29289,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"cd66a03e-9399-4671-a7e4-cea912a361d2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["save_path = \"/content/drive/My Drive/Colab Notebooks/\""],"metadata":{"id":"h9RE7SSamfg_","executionInfo":{"status":"ok","timestamp":1642516343844,"user_tz":-60,"elapsed":7,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"virpP5iPmkZs","executionInfo":{"status":"ok","timestamp":1642516373985,"user_tz":-60,"elapsed":411,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"d21c563b-6b00-4b41-8dcc-efcf242d7d49"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["# checking if we have access to a GPU\n","\n","tf.config.list_physical_devices()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2j4PiF_6mtQm","executionInfo":{"status":"ok","timestamp":1642516373999,"user_tz":-60,"elapsed":26,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"8f695c27-1695-4457-b333-520358bff685"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n"," PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["#@title Data Loading & Preprocessing\n"],"metadata":{"id":"S3sMA5T_mtkz","executionInfo":{"status":"ok","timestamp":1642516440274,"user_tz":-60,"elapsed":318,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"gj9DCuWKsl1l","executionInfo":{"status":"ok","timestamp":1642516548846,"user_tz":-60,"elapsed":966,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}}},"outputs":[],"source":["with open(save_path+'kafka_data_preprocessed_high_occurrence_50.json') as f:\n","    data = json.load(f)\n","    # shuffle data\n","    random.shuffle(data)\n","    # create description and assignee list\n","    desc_data = []\n","    assignee_data = []\n","    for item in data:\n","        desc_data.append(item['description'])\n","        assignee_data.append(item['assignee'])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0bpi1aHWsl1m","executionInfo":{"status":"ok","timestamp":1642516608429,"user_tz":-60,"elapsed":282,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"368eabcd-b46b-4f85-d001-1ba3d98e77a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["sample assignees: ['Jason Gustafson', 'A. Sophie Blee-Goldman']\n","sample descriptions: ['For KIP-320, we changed the permissions of the OffsetsForLeaderEpoch to be topic-level so that consumers did not require Cluster permission. However, there is no way for a consumer to know whether the broker is new enough to support this permission scheme. The only way to be sure is to use the version of this API that was bumped in 2.3. For older versions, we should revert to the old behavior.', \"To avoid triggering auto topic creation (if `auto.create.topic.enable=true` on the brokers), Kafka Streams uses consumer pattern subscription. For this case, the consumer requests all metadata from the brokers and does client side filtering. However, if users want to set ACL to restrict a Kafka Streams application, this may results in broker side ERROR logs that some metadata cannot be provided. The only way to avoid those broker side ERROR logs is to grant corresponding permissions. As of 2.3 release it's possible to disable auto topic creation client side (via https://issues.apache.org/jira/browse/KAFKA-7320). Kafka Streams should use this new feature (note, that broker version 0.11 is required) to allow users to set strict ACLs without getting flooded with ERROR logs on the broker. The proposal is that by default Kafka Streams disables auto-topic create client side (optimistically) and uses regular subscription (not pattern subscription). If an older broker is used, users need to explicitly enable `allow.auto.create.topic` client side. If we detect this setting, we switch back to pattern based subscription. If users don't enable auto topic create client side and run with an older broker, we would just rethrow the exception to the user, adding some context information on how to fix the issue.\"]\n"]}],"source":["print('sample assignees:', assignee_data[0:2])\n","print('sample descriptions:', desc_data[0:2])"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhnwNBlosl1n","executionInfo":{"status":"ok","timestamp":1642516620189,"user_tz":-60,"elapsed":496,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"30d8c407-eccf-45cb-a9c5-2aa4e34a67f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(b'For KIP-320, we changed the permissions of the OffsetsForLeaderEpoch to be topic-level so that consumers did not require Cluster permission. However, there is no way for a consumer to know whether the broker is new enough to support this permission scheme. The only way to be sure is to use the version of this API that was bumped in 2.3. For older versions, we should revert to the old behavior.', shape=(), dtype=string)\n","tf.Tensor(b\"To avoid triggering auto topic creation (if `auto.create.topic.enable=true` on the brokers), Kafka Streams uses consumer pattern subscription. For this case, the consumer requests all metadata from the brokers and does client side filtering. However, if users want to set ACL to restrict a Kafka Streams application, this may results in broker side ERROR logs that some metadata cannot be provided. The only way to avoid those broker side ERROR logs is to grant corresponding permissions. As of 2.3 release it's possible to disable auto topic creation client side (via https://issues.apache.org/jira/browse/KAFKA-7320). Kafka Streams should use this new feature (note, that broker version 0.11 is required) to allow users to set strict ACLs without getting flooded with ERROR logs on the broker. The proposal is that by default Kafka Streams disables auto-topic create client side (optimistically) and uses regular subscription (not pattern subscription). If an older broker is used, users need to explicitly enable `allow.auto.create.topic` client side. If we detect this setting, we switch back to pattern based subscription. If users don't enable auto topic create client side and run with an older broker, we would just rethrow the exception to the user, adding some context information on how to fix the issue.\", shape=(), dtype=string)\n"]}],"source":["# transform description data to tensorflow dataset\n","descriptions = tf.data.Dataset.from_tensor_slices(desc_data)\n","for input in descriptions.take(2):\n","    print(input)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Ftwycc5sl1o","executionInfo":{"status":"ok","timestamp":1642516627774,"user_tz":-60,"elapsed":226,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"97fbf90e-4f81-453b-e846-2713bad714ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'A. Sophie Blee-Goldman': 26,\n"," 'Apurva Mehta': 33,\n"," 'Ashish Singh': 7,\n"," 'Bill Bejeck': 6,\n"," 'Boyang Chen': 30,\n"," 'Bruno Cadonna': 23,\n"," 'Chia-Ping Tsai': 4,\n"," 'Chris Egerton': 29,\n"," 'Colin McCabe': 34,\n"," 'Damian Guy': 32,\n"," 'David Jacot': 20,\n"," 'Dong Lin': 9,\n"," 'Dongjin Lee': 19,\n"," 'Eno Thereska': 2,\n"," 'Ewen Cheslack-Postava': 5,\n"," 'Grant Henke': 25,\n"," 'Guozhang Wang': 14,\n"," 'Gwen Shapira': 0,\n"," 'Harsha': 8,\n"," 'Ismael Juma': 1,\n"," 'Jason Gustafson': 28,\n"," 'Jay Kreps': 31,\n"," 'Jiangjie Qin': 18,\n"," 'John Roesler': 10,\n"," 'Jun Rao': 16,\n"," 'Konstantine Karantasis': 24,\n"," 'Luke Chen': 3,\n"," 'Manikumar': 27,\n"," 'Matthias J. Sax': 21,\n"," 'Neha Narkhede': 22,\n"," 'Rajini Sivaram': 12,\n"," 'Randall Hauch': 17,\n"," 'Swapnil Ghike': 11,\n"," 'Tom Bentley': 13,\n"," 'Vahid Hashemian': 35,\n"," 'huxihx': 15}"]},"metadata":{},"execution_count":10}],"source":["# target vectorize assignee data\n","assignee_dict = {assignee: i for i, assignee in enumerate(list(set(assignee_data)))}\n","assignee_dict\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMpeS3_rsl1p","executionInfo":{"status":"ok","timestamp":1642516645043,"user_tz":-60,"elapsed":454,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"3219b3f6-bbfc-47ba-aee7-da75031b03ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[28, 26, 11, 17, 34, 1, 7, 16, 31, 22]"]},"metadata":{},"execution_count":11}],"source":["# create target vector\n","assignee_vector = [assignee_dict[assignee] for assignee in assignee_data]\n","assignee_vector[:10]"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58XXBC2csl1q","executionInfo":{"status":"ok","timestamp":1642516652833,"user_tz":-60,"elapsed":239,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"0c0b9621-b9b7-459d-a0e6-d01ca00333b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], shape=(36,), dtype=float32)\n","tf.Tensor(\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(36,), dtype=float32)\n","tf.Tensor(\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(36,), dtype=float32)\n","tf.Tensor(\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(36,), dtype=float32)\n","tf.Tensor(\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], shape=(36,), dtype=float32)\n"]}],"source":["# assignee target vector to one hot vector\n","assignee_tensor = tf.data.Dataset.from_tensor_slices(assignee_vector)\n","assignee_one_hot = assignee_tensor.map(lambda x: tf.one_hot(x, len(assignee_dict)))\n","for assignee in assignee_one_hot.take(5):\n","    print(assignee)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jBor5vzsl1q","executionInfo":{"status":"ok","timestamp":1642516668509,"user_tz":-60,"elapsed":348,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"275be258-25d8-4ac3-f068-18575543d893"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(b'For KIP-320, we changed the permissions of the OffsetsForLeaderEpoch to be topic-level so that consumers did not require Cluster permission. However, there is no way for a consumer to know whether the broker is new enough to support this permission scheme. The only way to be sure is to use the version of this API that was bumped in 2.3. For older versions, we should revert to the old behavior.', shape=(), dtype=string)\n","tf.Tensor(\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], shape=(36,), dtype=float32)\n","tf.Tensor(b\"To avoid triggering auto topic creation (if `auto.create.topic.enable=true` on the brokers), Kafka Streams uses consumer pattern subscription. For this case, the consumer requests all metadata from the brokers and does client side filtering. However, if users want to set ACL to restrict a Kafka Streams application, this may results in broker side ERROR logs that some metadata cannot be provided. The only way to avoid those broker side ERROR logs is to grant corresponding permissions. As of 2.3 release it's possible to disable auto topic creation client side (via https://issues.apache.org/jira/browse/KAFKA-7320). Kafka Streams should use this new feature (note, that broker version 0.11 is required) to allow users to set strict ACLs without getting flooded with ERROR logs on the broker. The proposal is that by default Kafka Streams disables auto-topic create client side (optimistically) and uses regular subscription (not pattern subscription). If an older broker is used, users need to explicitly enable `allow.auto.create.topic` client side. If we detect this setting, we switch back to pattern based subscription. If users don't enable auto topic create client side and run with an older broker, we would just rethrow the exception to the user, adding some context information on how to fix the issue.\", shape=(), dtype=string)\n","tf.Tensor(\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(36,), dtype=float32)\n"]}],"source":["# zip descriptions and assignee for training\n","dataset = tf.data.Dataset.zip((descriptions, assignee_one_hot))\n","for input, target in dataset.take(2):\n","    print(input)\n","    print(target)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"RGUGL0D9sl1r","executionInfo":{"status":"ok","timestamp":1642516676576,"user_tz":-60,"elapsed":244,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}}},"outputs":[],"source":["# shuffle dataset\n","data_len = len(dataset)\n","dataset = dataset.shuffle(data_len)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w_NtasBosl1r","executionInfo":{"status":"ok","timestamp":1642516681860,"user_tz":-60,"elapsed":256,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"cb950f9a-58eb-47ae-c196-40344fcb7aae"},"outputs":[{"output_type":"stream","name":"stdout","text":["n_examples: 4338\n"]}],"source":["# split dataset into train val and test\n","BATCH_SIZE = 32\n","VAL_SIZE = 0.15\n","n_examples = tf.data.experimental.cardinality(dataset).numpy()\n","print('n_examples:', n_examples)\n","\n","VAL_SIZE = round(VAL_SIZE*n_examples)\n","test_dataset = dataset.take(VAL_SIZE).batch(batch_size=BATCH_SIZE, drop_remainder=True)\n","val_dataset = dataset.skip(VAL_SIZE).take(VAL_SIZE).batch(batch_size=BATCH_SIZE, drop_remainder=True)\n","train_dataset = dataset.skip(VAL_SIZE*2).batch(batch_size=BATCH_SIZE, drop_remainder=True)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3r5S5BGsl1s","executionInfo":{"status":"ok","timestamp":1642516690725,"user_tz":-60,"elapsed":225,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"fe9f015a-b257-43ed-cb55-0f124470aeda"},"outputs":[{"output_type":"stream","name":"stdout","text":["len(train_dataset): 94\n","len(val_dataset): 20\n","len(test_dataset): 20\n"]}],"source":["# prefetch train val and test dataset\n","train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","print('len(train_dataset):', len(train_dataset))\n","print('len(val_dataset):', len(val_dataset))\n","print('len(test_dataset):', len(test_dataset))\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jt2P1QTmsl1t","executionInfo":{"status":"ok","timestamp":1642516714042,"user_tz":-60,"elapsed":389,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"d2200b6c-ea11-473e-91dc-685bf7f54c0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[b\"Users often want to read topic multiple times. However, this is not possible because there is a single consumer and thus a topic can only be consumed once. Users get an exception {quote}Exception in thread \\xe2\\x80\\x9cmain\\xe2\\x80\\x9d org.apache.kafka.streams.errors.TopologyException: Invalid topology: Topic source has already been registered by another source. {quote} If they use a topic name in multiple `stream()`, `table()`, `globalTable()` calls. However, with KAFKA-6034 in place, we could allow adding a topic multiple times and rewrite the topology internally to only read the topic once. This would simplify application code as users don't need to put workaround in place to get the same behavior.\"\n"," b'Currently it does not appear that the KafkaShortnamer allows for case insensitive search and replace rules. It would be good to match the functionality provided by HDFS as operators are familiar with this. This also makes it easier to port auth_to_local rules from your existing hdfs configurations to your new kafka configuration. HWX auth_to_local guide for referencehttps://community.hortonworks.com/articles/14463/auth-to-local-rules-syntax.html'\n"," b\"_max.task.idle.ms_ is an handy way to pause processing in a *_kafka-streams_* application. This is very useful when you need to join two topics that are out of sync, i.e when data in a topic may be produced _before_ you receive join information in the other topic. In the documentation, however, it is not specified that the value of _max.task.idle.ms_ *must* be lower than _max.poll.intervall.ms_, otherwise you'll incur into an endless rebalancing problem. I think it is better to clearly state this in the documentation.\"\n"," b'Currently we close the stream thread in the following way:0. Commit all tasks.1. Close producer.2. Close consumer.3. Close restore consumer.4. For each task, close its topology processors one-by-one following the topology order by calling {{processor.close()}}.5. For each task, close its state manager as well as flushing and closing all its associated state stores.We choose to close the producer / consumer clients before shutting down the tasks because we need to make sure all sent records has been acked so that we have the right log-end-offset when closing the store and checkpointing the offset of the changelog. However there is also an issue with this ordering, in which users choose to write more records in their {{processor.close()}} calls, this will cause RTE since the producers has already been closed, and no changelog records will be able to write.Thinking about this issue, a more appropriate ordering will be:1. For each task, close their topology processors following the topology order by calling {{processor.close()}}.2. For each task, commit its state by calling {{task.commit()}}. At this time all sent records should be acked since {{producer.flush()}} is called.3. For each task, close their {{ProcessorStateManager}}.4. Close all embedded clients, i.e. producer / consumer / restore consumer.'\n"," b\"It's useful to have a max-messages option on the SimpleConsumerShell similar to other tools.\"\n"," b'This issue happens in Trunk only.** To reproduce the issue:1. Start zookeeper: $ bin/zookeeper-server-start.sh config/zookeeper.properties2. Start broker: $ bin/kafka-server-start.sh config/server.properties3. Start producer: $ bin/kafka-run-class.sh kafka.perf.ProducerPerformance --broker-list localhost:9092 --messages 100 --topics topic_001 --batch-size 50 --threads 1 --message-size 200** Broker exception:[2013-01-18 10:11:41,241] ERROR Replica Manager on Broker 0: Error processing leaderAndISR request LeaderAndIsrRequest(0,5,,1000,Map((topic_001,0) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ \"ISR\":\"0\",\"leader\":\"0\",\"leaderEpoch\":\"0\" },1),1)),Set(id:0,host:jfung-ld.linkedin.biz,port:9092),1) (kafka.server.ReplicaManager)java.lang.NullPointerException at kafka.log.OffsetIndex.entries(OffsetIndex.scala:285) at kafka.log.OffsetIndex$$anonfun$1.apply(OffsetIndex.scala:88) at kafka.log.OffsetIndex$$anonfun$1.apply(OffsetIndex.scala:88) at kafka.utils.Logging$class.info(Logging.scala:67) at kafka.log.OffsetIndex.info(OffsetIndex.scala:52) at kafka.log.OffsetIndex.<init>(OffsetIndex.scala:87) at kafka.log.LogSegment.<init>(LogSegment.scala:37) at kafka.log.Log.loadSegments(Log.scala:132) at kafka.log.Log.<init>(Log.scala:75) at kafka.log.LogManager.createLogIfNotExists(LogManager.scala:202) at kafka.log.LogManager.getOrCreateLog(LogManager.scala:179) at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:77) at kafka.cluster.Partition$$anonfun$1.apply(Partition.scala:142) at kafka.cluster.Partition$$anonfun$1.apply(Partition.scala:142) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61) at scala.collection.immutable.List.foreach(List.scala:45) at scala.collection.TraversableLike$class.map(TraversableLike.scala:206) at scala.collection.immutable.List.map(List.scala:45) at kafka.cluster.Partition.makeLeader(Partition.scala:142) at kafka.server.ReplicaManager.kafka$server$ReplicaManager$$makeLeader(ReplicaManager.scala:219) at kafka.server.ReplicaManager$$anonfun$becomeLeaderOrFollower$3.apply(ReplicaManager.scala:199) at kafka.server.ReplicaManager$$anonfun$becomeLeaderOrFollower$3.apply(ReplicaManager.scala:191) at scala.collection.immutable.Map$Map1.foreach(Map.scala:105) at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:191) at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:155) at kafka.server.KafkaApis.handle(KafkaApis.scala:65) at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41) at java.lang.Thread.run(Thread.java:662)** ProducerPerformance exception:[2013-01-18 10:07:02,779] ERROR Error in handling batch of 50 events (kafka.producer.async.ProducerSendThread)kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries. at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:87) at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67) at scala.collection.immutable.Stream.foreach(Stream.scala:254) at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66) at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)'\n"," b'I\\'m getting this deadlock on half of Kafka Connect runs when having two different types connectors (in this configuration it\\'s debezium and hdfs). Thread 1: {code} \"pool-22-thread-2@4748\" prio=5 tid=0x4d nid=NA waiting for monitor entry java.lang.Thread.State: BLOCKED waiting for pool-22-thread-1@4747 to release lock on <0x1423> (a org.apache.kafka.connect.runtime.isolation.PluginClassLoader) at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:91) at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.loadClass(DelegatingClassLoader.java:367) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Class.java:-1) at java.lang.Class.forName(Class.java:348) at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:715) at org.apache.kafka.connect.runtime.ConnectorConfig.enrich(ConnectorConfig.java:295) at org.apache.kafka.connect.runtime.ConnectorConfig.<init>(ConnectorConfig.java:200) at org.apache.kafka.connect.runtime.ConnectorConfig.<init>(ConnectorConfig.java:194) at org.apache.kafka.connect.runtime.Worker.startConnector(Worker.java:233) at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startConnector(DistributedHerder.java:916) at org.apache.kafka.connect.runtime.distributed.DistributedHerder.access$1300(DistributedHerder.java:111) at org.apache.kafka.connect.runtime.distributed.DistributedHerder$15.call(DistributedHerder.java:932) at org.apache.kafka.connect.runtime.distributed.DistributedHerder$15.call(DistributedHerder.java:928) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) {code} Thread 2: {code} \"pool-22-thread-1@4747\" prio=5 tid=0x4c nid=NA waiting for monitor entry java.lang.Thread.State: BLOCKED blocks pool-22-thread-2@4748 waiting for pool-22-thread-2@4748 to release lock on <0x1421> (a org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) at java.lang.ClassLoader.loadClass(ClassLoader.java:406) at org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader.loadClass(DelegatingClassLoader.java:358) at java.lang.ClassLoader.loadClass(ClassLoader.java:411) - locked <0x1424> (a java.lang.Object) at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:104) - locked <0x1423> (a org.apache.kafka.connect.runtime.isolation.PluginClassLoader) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at io.debezium.transforms.ByLogicalTableRouter.<clinit>(ByLogicalTableRouter.java:57) at java.lang.Class.forName0(Class.java:-1) at java.lang.Class.forName(Class.java:348) at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:715) at org.apache.kafka.connect.runtime.ConnectorConfig.enrich(ConnectorConfig.java:295) at org.apache.kafka.connect.runtime.ConnectorConfig.<init>(ConnectorConfig.java:200) at org.apache.kafka.connect.runtime.ConnectorConfig.<init>(ConnectorConfig.java:194) at org.apache.kafka.connect.runtime.Worker.startConnector(Worker.java:233) at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startConnector(DistributedHerder.java:916) at org.apache.kafka.connect.runtime.distributed.DistributedHerder.access$1300(DistributedHerder.java:111) at org.apache.kafka.connect.runtime.distributed.DistributedHerder$15.call(DistributedHerder.java:932) at org.apache.kafka.connect.runtime.distributed.DistributedHerder$15.call(DistributedHerder.java:928) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) {code} I\\'m using official Confluent Docker images.'\n"," b'A couple of places we can improve the upgrade docs:1) Explanation about replica.lag.time.max.ms and how it relates to the old configs.2) Default quota configs.3) Client-server compatibility: old clients working with new servers and new clients working with old servers?'\n"," b'It would be nice to have Kafka brokers auto-assign node ids rather than having that be a configuration. Having a configuration is irritating because (1) you have to generate a custom config for each broker and (2) even though it is in configuration, changing the node id can cause all kinds of bad things to happen.'\n"," b'We fixed the error code handling on producer in https://issues.apache.org/jira/browse/KAFKA-10687, however the newly thrown `InvalidProducerEpoch` exception was not properly handled on Streams side in all cases. We should catch it and rethrow as TaskMigrated to trigger exception, similar to ProducerFenced.'\n"," b\"to pickup the following useful change:ZkEventThread shouldn't stop on errors - catch throwables as well\"\n"," b'Placeholder until API finalized'\n"," b'{code}kafka.admin.BrokerApiVersionsCommandTest > checkBrokerApiVersionCommandOutput FAILED org.junit.ComparisonFailure: expected:<[localhost:34091 (id: 0 rack: null) -> (]> but was:<[]> at org.junit.Assert.assertEquals(Assert.java:115) at org.junit.Assert.assertEquals(Assert.java:144) at kafka.admin.BrokerApiVersionsCommandTest.checkBrokerApiVersionCommandOutput(BrokerApiVersionsCommandTest.scala:44){code}'\n"," b\"Currently if mirror maker got shutdown uncleanly, the data in the data channel and buffer could potentially be lost. With the new producer's callback, this issue could be solved.\"\n"," b'So that1. It can check if rebalances are necessary when topic/partition watcher fires (they can be triggered for state change event even the data does not change at all).2. Rebalance does not need to read again from ZK.'\n"," b'When launching a KafkaStreams application that depends on a topic that doesn\\'t exist, the streams application correctly logs an error such as: \"<TOPIC_NAME> is unknown yet during rebalance, please make sure they have been pre-created before starting the Streams application.\" The stream is then shutdown, however, no exception is thrown indicating that an error has occurred. In our circumstances, we run our streams app inside a container. The streams service is shutdown, but the process is not exited, meaning that the container does not crash (reducing visibility of the issue). As no exception is thrown in the missing topic scenario described above, our application code has no way to determine that something is wrong that would then allow it to terminate the process. Could the onPartitionsAssigned method in StreamThread.java throw an exception when it decides to shutdown the stream (somewhere around line 264)?'\n"," b'Currently we have two log retention strategies: one based on time and one based on log size. These work well for \"event\" type data--i.e. data that consists only of appends. However if the events model changes to an underlying keyed data set, a more convenient retention strategy would delete keys that had been overwritten rather than retaining whole segments.The proposed implementation would be a background process that scanned log segments and recopied only keys that hadn\\'t been overwritten. Some more details are in this wiki:https://cwiki.apache.org/confluence/display/KAFKA/Keyed+Messages+Proposal'\n"," b'From recent failed build:{code}org.apache.kafka.connect.runtime.WorkerSourceTaskTest > testCommit FAILED java.lang.AssertionError: Expectation failure on verify: Listener.onStartup(job-0): expected: 1, actual: 1 Listener.onShutdown(job-0): expected: 1, actual: 1 at org.easymock.internal.MocksControl.verify(MocksControl.java:225) at org.powermock.api.easymock.internal.invocationcontrol.EasyMockMethodInvocationControl.verify(EasyMockMethodInvocationControl.java:132) at org.powermock.api.easymock.PowerMock.verify(PowerMock.java:1466) at org.powermock.api.easymock.PowerMock.verifyAll(PowerMock.java:1405) at org.apache.kafka.connect.runtime.WorkerSourceTaskTest.testCommit(WorkerSourceTaskTest.java:221){code}'\n"," b'Found this failure in https://builds.apache.org/job/kafka-trunk-git-pr-jdk7/4288/testReport/junit/org.apache.kafka.connect.runtime/WorkerSourceTaskTest/testSlowTaskStart/{code}https://builds.apache.org/job/kafka-trunk-git-pr-jdk7/4288/testReport/junit/org.apache.kafka.connect.runtime/WorkerSourceTaskTest/testSlowTaskStart/{code}'\n"," b'If there is an unclean leader election, a follower may have an offset out of the range of the leader. Currently, the follower will delete all its data and refetch from the smallest offset of the leader. It would be useful to add an option to let the follower refetch from the largest offset of the leader since refetching from the smallest offset may take some time.'\n"," b'As an optimization, if a transaction is aborted, we can drop any records which have not yet been sent to the brokers. However, to avoid the sequence number getting out of sync, we need to continue sending any request which has been sent at least once.'\n"," b'I have next code:{code}Properties props = new Properties();props.put(\"metadata.broker.list\", Services.getConfigInstance().getKafkaBrokerList());props.put(\"serializer.class\", \"main.java.com.services.kafka.MessageEntityToJsonSerializer\");props.put(\"request.required.acks\", \"0\");ProducerConfig config = new ProducerConfig(props);if (log.isLoggable(Level.INFO)) { log.log(Level.INFO, \"Connecting to Kafka...props: \" + props);}producer = new Producer<String, MessageEntity>(config);if (log.isLoggable(Level.INFO)) { log.log(Level.INFO, \"Connected to Kafka\");}{code}It just hungs on \\'new Producer\\' in case of wrong \\'serializer.class\\' property'\n"," b'java.lang.AssertionError: Expected partitions [topic-0, topic-1, topic2-0, topic2-1] but actually got [topic-0, topic-1] at org.junit.Assert.fail(Assert.java:88) at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:730) at kafka.api.BaseConsumerTest.testAutoCommitOnRebalance(BaseConsumerTest.scala:125) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:22'\n"," b'While analysing https://issues.apache.org/jira/browse/KAFKA-7965, we found that multiple members of a group can be evicted from a group if the leader of the consumer offset partition changes before the group is persisted. This happens because the current evection logic always evict the first member which rejoins the group. We would like to change the evection logic so that the last members to rejoin the group are kicked out instead. Here is an example of what happens when the leader changes: {noformat} // Group is loaded in GroupCoordinator 0 // A rebalance is triggered because the group is over capacity [2020-04-02 11:14:33,393] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager:66) [2020-04-02 11:14:33,406] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Discovered group coordinator localhost:40071 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:794) [2020-04-02 11:14:33,409] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3 at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:33,410] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:33,412] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:33,413] INFO [GroupCoordinator 0]: Loading group metadata for group-max-size-test with generation 1 (kafka.coordinator.group.GroupCoordinator:66) [2020-04-02 11:14:33,413] INFO [GroupCoordinator 0]: Preparing to rebalance group group-max-size-test in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: Freshly-loaded group is over capacity (GroupConfig(10,1800000,2,0).groupMaxSize). Rebalacing in order to give a chance for consumers to commit offsets) (kafka.coordinator.group.GroupCoordinator:66) [2020-04-02 11:14:33,431] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 28 milliseconds, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66) // A first consumer is kicked out of the group while trying to re-join [2020-04-02 11:14:33,449] ERROR [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Attempt to join group failed due to fatal error: The consumer group has reached its max size. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:627) [2020-04-02 11:14:33,451] ERROR [daemon-consumer-assignment-2]: Error due to (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:76) org.apache.kafka.common.errors.GroupMaxSizeReachedException: Consumer group group-max-size-test already has the configured maximum number of members. [2020-04-02 11:14:33,451] INFO [daemon-consumer-assignment-2]: Stopped (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:66) // Before the rebalance is completed, a preferred replica leader election kicks in and move the leader from 0 to 1 [2020-04-02 11:14:34,155] INFO [Controller id=0] Processing automatic preferred replica leader election (kafka.controller.KafkaController:66) [2020-04-02 11:14:34,169] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions group-max-size-test-0,group-max-size-test-3,__consumer_offsets-0 triggered by AutoTriggered (kafka.controller.KafkaController:66) // The group is loaded in GroupCoordinator 1 before completing the rebalance // Another rebalance is triggered because the group is still over capacity [2020-04-02 11:14:34,194] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager:66) [2020-04-02 11:14:34,199] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3 at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:34,199] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:34,199] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:34,201] INFO [GroupCoordinator 1]: Loading group metadata for group-max-size-test with generation 1 (kafka.coordinator.group.GroupCoordinator:66) [2020-04-02 11:14:34,202] INFO [GroupCoordinator 1]: Preparing to rebalance group group-max-size-test in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: Freshly-loaded group is over capacity (GroupConfig(10,1800000,2,0).groupMaxSize). Rebalacing in order to give a chance for consumers to commit offsets) (kafka.coordinator.group.GroupCoordinator:66) [2020-04-02 11:14:34,203] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 9 milliseconds, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66) // Prefered leader election is completed [2020-04-02 11:14:34,235] INFO [Controller id=0] Partition __consumer_offsets-0 completed preferred replica leader election. New leader is 1 (kafka.controller.KafkaController:66) // Group is unloaded from GroupCoordinator 0 [2020-04-02 11:14:34,237] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager:66) [2020-04-02 11:14:34,237] INFO [GroupCoordinator 0]: Unloading group metadata for group-max-size-test with generation 1 (kafka.coordinator.group.GroupCoordinator:66) [2020-04-02 11:14:34,238] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-0. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager:66) // A second consumer is kicked out of the group while trying to re-join [2020-04-02 11:14:34,252] ERROR [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Attempt to join group failed due to fatal error: The consumer group has reached its max size. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:627) [2020-04-02 11:14:34,254] ERROR [daemon-consumer-assignment-1]: Error due to (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:76) org.apache.kafka.common.errors.GroupMaxSizeReachedException: Consumer group group-max-size-test already has the configured maximum number of members. [2020-04-02 11:14:34,254] INFO [daemon-consumer-assignment-1]: Stopped (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:66) {noformat}'\n"," b'When cleaning the log, we don\\'t want to convert messages to the format configured for the topic due to KAFKA-3915. However, the cleaner logic for writing compressed messages (in case some messages in the message set were not retained) writes the topic message format version in the magic field of the outer message instead of the actual message format. The choice of the absolute/relative offset for the inner messages will also be based on the topic message format version.For example, if there is an old compressed message set with magic=0 in the log and the topic is configured for magic=1, then after cleaning, the new message set will have a wrapper with magic=1, the nested messages will still have magic=0, but the message offsets will be relative. If this happens, there does not seem to be an easy way to recover without manually fixing up the log.The offsets still work correctly as both the clients and broker use the outer message format version to decide if the relative offset needs to be converted to an absolute offset. So the main problem turns out to be that `ByteBufferMessageSet.deepIterator` throws an exception if there is a mismatch between outer and inner message format version.{code}if (newMessage.magic != wrapperMessage.magic) throw new IllegalStateException(s\"Compressed message has magic value ${wrapperMessage.magic} \" + s\"but inner message has magic value ${newMessage.magic}\"){code}'\n"," b'If you run \"./gradlew javadoc\", you will only get JavaDoc for the High Level Consumer. All the new clients are missing.See here: http://home.apache.org/~gwenshap/0.10.0.0-rc5/javadoc/I suggest fixing in 0.10.0 branch and in trunk, not rolling a new release candidate, but updating our docs site.'\n"," b'Mirror maker hangs indefinitely upon receiving CommitFailedException. I believe this is due to CommitFailedException not caught by mirror maker and mirror maker has no way to recover from it.A better approach will be catching the exception and rejoin the group. Here is the stack trace[2016-03-15 09:34:36,463] ERROR Error UNKNOWN_MEMBER_ID occurred while committing offsets for group xxxxx (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)[2016-03-15 09:34:36,463] FATAL [mirrormaker-thread-3] Mirror maker thread failure due to (kafka.tools.MirrorMaker$MirrorMakerThread)org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed due to group rebalance at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:552) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:493) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:665) at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:644) at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167) at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133) at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:380) at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:274) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:320) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:213) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:193) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163) at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:358) at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:968) at kafka.tools.MirrorMaker$MirrorMakerNewConsumer.commit(MirrorMaker.scala:548) at kafka.tools.MirrorMaker$.commitOffsets(MirrorMaker.scala:340) at kafka.tools.MirrorMaker$MirrorMakerThread.maybeFlushAndCommitOffsets(MirrorMaker.scala:438) at kafka.tools.MirrorMaker$MirrorMakerThread.run(MirrorMaker.scala:399)[2016-03-15 09:34:36,463] INFO [mirrormaker-thread-3] Flushing producer. (kafka.tools.MirrorMaker$MirrorMakerThread)[2016-03-15 09:34:36,464] INFO [mirrormaker-thread-3] Committing consumer offsets. (kafka.tools.MirrorMaker$MirrorMakerThread)[2016-03-15 09:34:36,477] ERROR Error UNKNOWN_MEMBER_ID occurred while committing offsets for group xxxxx (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)'\n"," b'When the [final cleanup for a sink task|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java#L191] is initiated, the framework [first calls stop() on the task|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L167], and then [closes the consumer for the task|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L171]. Closing the consumer has the side effect of triggering [the onPartitionsRevoked method|https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/consumer/ConsumerRebalanceListener.html#onPartitionsRevoked-java.util.Collection-] of its {{ConsumerRebalanceListener}}, which in turn [causes the framework to call WorkerSinkTask::closePartitions|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L694], which in turn [calls WorkerSinkTask::commitOffsets|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L618], and finally, in turn [calls SinkTask:preCommit|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L386]. Calling {{SinkTask:preCommit}} after {{SinkTask::stop}} is likely to cause errors with tasks as they should be performing resource cleanup during {{stop}}, and the [current documentation on the SinkTask lifecycle|https://kafka.apache.org/25/javadoc/org/apache/kafka/connect/sink/SinkTask.html] makes no mention of anything happening after tasks are stopped. The framework already [ensures that offsets are committed|https://github.com/apache/kafka/blob/3c43adff1d4562c6b33732f399691c9e2f887903/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L196] for tasks after the last call to {{SinkTask:put}} has been made, so the offset commit after {{SinkTask::stop}} has already been invoked can and should be removed with no compromise of existing delivery guarantees provided by the framework.'\n"," b'Among others, Streams reports the following two thread-level \"process\" metrics: \"process-rate\": The average number of process calls per second. \"process-total\": The total number of process calls across all tasks. See the docs: https://kafka.apache.org/documentation/#kafka_streams_thread_monitoring There\\'s some surprising ambiguity in these definitions that has led to Streams actually reporting something different than what most people would probably expect. Specifically, it\\'s not defined what a \"process call\" is. A reasonable definition of a \"process call\" is processing a record or processing a task (both of which are publicly facing concepts, and both of which are the same, since tasks process records one at a time). However, we currently measure number of invocations to a private, internal `process()` method, which would actually process more than one record at a time. Thus, the current metric is under-counting the throughput, in an esoteric and confusing way. Instead, we should simply change the rate and total metrics to measure the (rate and total) of _record_ processing.'\n"," b'Store iterators should be closed in all/most of the cases, but currently it is not consistently reflected in the documentation and javadocs. For instance https://kafka.apache.org/0110/documentation/streams/developer-guide#streams_developer-guide_interactive-queries_custom-stores does not mention the need to close an iterator and provide an example that does not do that. Some of the fetch methods do mention the need to close an iterator returned (e.g. https://kafka.apache.org/0110/javadoc/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.html#range(K,%20K)), but others do not: https://kafka.apache.org/0110/javadoc/org/apache/kafka/streams/state/ReadOnlyWindowStore.html#fetch(K,%20long,%20long)It makes sense to: - update javadoc for all store methods that do return iterators to reflect that the iterator returned needs to be closed- mention it in the documentation and to update related examples.'\n"," b'The replication-offset-checkpoint file seems to have some blank entries where the topic, partition, offset information is expected.Broker service cannot be started with following error:[2014-06-18 11:09:51,272] ERROR [KafkaApi-5] error when handling request Name:LeaderAndIsrRequest;Version:0;Controller:2;ControllerEpoch:35;CorrelationId:11;ClientId:id_2-host_10.65.127.89-port_9092;PartitionState:(x,4) -> (LeaderAndIsrInfo:(Leader:14,ISR:14,LeaderEpoch:33,ControllerEpoch:35),ReplicationFactor:3),AllReplicas:14,5,6),(xm,20) -> (LeaderAndIsrInfo:(Leader:11,ISR:11,4,LeaderEpoch:11,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:11,4,5),(xm,0) -> (LeaderAndIsrInfo:(Leader:11,ISR:11,4,LeaderEpoch:28,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:11,4,5),(xm_int,3) -> (LeaderAndIsrInfo:(Leader:17,ISR:17,18,LeaderEpoch:26,ControllerEpoch:30),ReplicationFactor:3),AllReplicas:5,17,18),(x_int,9) -> (LeaderAndIsrInfo:(Leader:11,ISR:11,LeaderEpoch:38,ControllerEpoch:35),ReplicationFactor:3),AllReplicas:11,5,6),(xm_int,11) -> (LeaderAndIsrInfo:(Leader:13,ISR:13,LeaderEpoch:29,ControllerEpoch:35),ReplicationFactor:3),AllReplicas:13,5,6),(x_int,3) -> (LeaderAndIsrInfo:(Leader:19,ISR:19,0,LeaderEpoch:25,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:5,19,0),(xm,1) -> (LeaderAndIsrInfo:(Leader:12,ISR:12,LeaderEpoch:30,ControllerEpoch:35),ReplicationFactor:3),AllReplicas:12,5,6),(x,3) -> (LeaderAndIsrInfo:(Leader:13,ISR:13,4,LeaderEpoch:22,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:13,4,5),(xm,14) -> (LeaderAndIsrInfo:(Leader:18,ISR:18,19,LeaderEpoch:24,ControllerEpoch:31),ReplicationFactor:3),AllReplicas:5,18,19),(xm_int,10) -> (LeaderAndIsrInfo:(Leader:12,ISR:12,4,LeaderEpoch:25,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:12,4,5),(x,15) -> (LeaderAndIsrInfo:(Leader:17,ISR:17,16,LeaderEpoch:24,ControllerEpoch:30),ReplicationFactor:3),AllReplicas:5,16,17),(x_int,8) -> (LeaderAndIsrInfo:(Leader:10,ISR:10,4,LeaderEpoch:26,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:10,4,5),(xm,21) -> (LeaderAndIsrInfo:(Leader:12,ISR:12,LeaderEpoch:8,ControllerEpoch:35),ReplicationFactor:3),AllReplicas:12,5,6);Leaders:id:11,host:25.126.81.157,port:9092,id:14,host:25.126.81.159,port:9092,id:12,host:25.126.81.158,port:9092,id:17,host:10.153.63.196,port:9092,id:18,host:10.153.63.214,port:9092,id:19,host:10.65.127.95,port:9092,id:13,host:10.65.127.93,port:9092,id:10,host:10.65.127.92,port:9092 (kafka.server.KafkaApis)java.lang.NumberFormatException: For input string: \" \" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Integer.parseInt(Integer.java:481) at java.lang.Integer.parseInt(Integer.java:527) at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:231) at scala.collection.immutable.StringOps.toInt(StringOps.scala:31) at kafka.server.OffsetCheckpoint.liftedTree2$1(OffsetCheckpoint.scala:77) at kafka.server.OffsetCheckpoint.read(OffsetCheckpoint.scala:73) at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:91) at kafka.server.ReplicaManager$$anonfun$makeFollowers$5.apply(ReplicaManager.scala:347) at kafka.server.ReplicaManager$$anonfun$makeFollowers$5.apply(ReplicaManager.scala:346) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:95) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:95) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157) at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:45) at scala.collection.mutable.HashMap.foreach(HashMap.scala:95) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.HashMap.map(HashMap.scala:45) at kafka.server.ReplicaManager.makeFollowers(ReplicaManager.scala:346) at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:248) at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:93) at kafka.server.KafkaApis.handle(KafkaApis.scala:74) at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:42) at java.lang.Thread.run(Thread.java:724)'\n"," b\"When a log entry is appended to a Kafka topic using KafkaLog4jAppender, the producer.send operation may block waiting for metadata. This can result in deadlocks in a couple of scenarios if a log entry from the producer network thread is also at a log level that results in the entry being appended to a Kafka topic. 1. Producer's network thread will attempt to send data to a Kafka topic and this is unsafe since producer.send may block waiting for metadata, causing a deadlock since the thread will not process the metadata request/response. 2. KafkaLog4jAppender#append is invoked while holding the lock of the logger. So the thread waiting for metadata in the initial send will be holding the logger lock. If the producer network thread has.a log entry that needs to be appended, it will attempt to acquire the logger lock and deadlock. This was probably the case right from the beginning when KafkaLog4jAppender was introduced, but did not cause any issues so far since there were only debug log entries in that path which were not logged to a Kafka topic by any of the tests. A recent info level log entry introduced by the commit https://github.com/apache/kafka/commit/a3aea3cf4dbedb293f2d7859e0298bebc8e2185f is causing system test failures in log4j_appender_test.py due to the deadlock. The asynchronous append case can be fixed by moving all send operations to a separate thread. But KafkaLog4jAppender also has a syncSend option which blocks append while holding the logger lock until the send completes. Not sure how this can be fixed if we want to support log appends from the producer network thread.\"], shape=(32,), dtype=string)\n","tf.Tensor(\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]], shape=(32, 36), dtype=float32)\n","tf.Tensor(\n","[b'This is an umbrella story for capturing changes to Kafka Streams to enable Queryable state as described in KIP-67 https://cwiki.apache.org/confluence/display/KAFKA/KIP-67%3A+Queryable+state+for+Kafka+Streams.'\n"," b'Pretty much as soon as we started actively monitoring the _last-rebalance-seconds-ago_ metric in our Kafka Streams test environment, we started seeing something weird. Every so often one of the StreamThreads (ie a single Consumer instance) would appear to permanently fall out of the group, as evidenced by a monotonically increasing _last-rebalance-seconds-ago._ We inject artificial network failures every few hours at most, so the group rebalances quite often. But the one consumer never rejoins, with no other symptoms (besides a slight drop in throughput since the remaining threads had to take over this member\\'s work). We\\'re confident that the problem exists in the client layer, since the logs confirmed that the unhealthy consumer was still calling poll. It was also calling Consumer#committed in its main poll loop, which was consistently failing with a TimeoutException. When I attached a remote debugger to an instance experiencing this issue, the network client\\'s connection to the group coordinator (the one that uses MAX_VALUE - node.id as the coordinator id) was in the DISCONNECTED state. But for some reason it never tried to re-establish this connection, although it did successfully connect to that same broker through the \"normal\" connection (ie the one that juts uses node.id). The tl;dr is that the AbstractCoordinator\\'s FindCoordinatorRequest has failed (presumably due to a disconnect), but the _findCoordinatorFuture_ is non-null so a new request is never sent. This shouldn\\'t be possible since the FindCoordinatorResponseHandler is supposed to clear the _findCoordinatorFuture_ when the future is completed. But somehow that didn\\'t happen, so the consumer continues to assume there\\'s still a FindCoordinator request in flight and never even notices that it\\'s dropped out of the group. These are the only confirmed findings so far, however we have some guesses which I\\'ll leave in the comments. Note that we only noticed this due to the newly added _last-rebalance-seconds-ago_ __metric, and there\\'s no reason to believe this bug hasn\\'t been flying under the radar since the Consumer\\'s inception'\n"," b'The quorum controller verifies the isr version from the `AlterIsr` request. If it does not match, then `INVALID_UPDATE_VERSION` is returned. At the moment, this prevents the leader from being able to shrink or expand the ISR since the version does not get propagated in `RaftReplicaChangeDelegate.makeLeaders`.'\n"," b'When using *group.instance.id=myUniqId[0]*, the KafkaConsumer\\'s constructor throws a NullpointerException in close(): {code:java} Caused by: java.lang.NullPointerException at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:2204) at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:825) at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:664) at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:644) {code} {{It turns out that the exception is thrown because the *log* member is not yet initialized (still null) in the constructor when the original exception is handled. The original exception is thrown before *log* is initailized.}} {{The side effect of this error is, that close does does not cleanup resources as clean is supposed to do.}} *{{The used consumer properties for reference:}}* {code:java} key.deserializer=com.ibm.streamsx.kafka.serialization.... request.timeout.ms=25000 value.deserializer=com.ibm.streamsx.kafka.serialization.... client.dns.lookup=use_all_dns_ips metadata.max.age.ms=2000 enable.auto.commit=false group.instance.id=myUniqId[0] max.poll.interval.ms=300000 group.id=consumer-0 metric.reporters=com.ibm.streamsx.kafka.clients.consum... reconnect.backoff.max.ms=10000 bootstrap.servers=localhost:9092 max.poll.records=50 session.timeout.ms=20000 client.id=C-J37-ReceivedMessages[0] allow.auto.create.topics=false metrics.sample.window.ms=10000 retry.backoff.ms=500 reconnect.backoff.ms=250{code} *Expected behaviour:* throw exception indicating that something is wrong with the chosen group.instance.id. The documentation does not tell anything about valid values for group.instance.id. *Reproduce:* {code:java} import java.util.Properties; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.clients.consumer.KafkaConsumer; public class Main { public static void main(String[] args) { Properties props = new Properties(); props.put (ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\"); props.put (ConsumerConfig.GROUP_ID_CONFIG, \"group-Id1\"); props.put (ConsumerConfig.GROUP_INSTANCE_ID_CONFIG, \"myUniqId[0]\"); props.put (ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put (ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer c = new KafkaConsumer (props); } } Exception in thread \"main\" java.lang.NullPointerException at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:2204) at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:825) at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:664) at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:644) at Main.main(Main.java:15) {code}'\n"," b'[https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/5168/consoleFull] *16:17:04* kafka.api.PlaintextConsumerTest > testLowMaxFetchSizeForRequestAndPartition FAILED*16:17:04* org.scalatest.exceptions.TestFailedException: Timed out before consuming expected 2700 records. The number consumed was 1980.*16:17:04* at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)*16:17:04* at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)*16:17:04* at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1389)*16:17:04* at org.scalatest.Assertions.fail(Assertions.scala:1091)*16:17:04* at org.scalatest.Assertions.fail$(Assertions.scala:1087)*16:17:04* at org.scalatest.Assertions$.fail(Assertions.scala:1389)*16:17:04* at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:789)*16:17:04* at kafka.utils.TestUtils$.pollRecordsUntilTrue(TestUtils.scala:765)*16:17:04* at kafka.api.AbstractConsumerTest.consumeRecords(AbstractConsumerTest.scala:156)*16:17:04* at kafka.api.PlaintextConsumerTest.testLowMaxFetchSizeForRequestAndPartition(PlaintextConsumerTest.scala:801)*16:17:04*'\n"," b'Currently, ControllerChannelManager maintains a queue per broker. If the queue fills up, metadata propagation to the broker is delayed. It would be useful to expose a metric on the size on the queue for monitoring.'\n"," b'Saw the following when running the new producer in ProducerPerformance.[2014-03-03 12:44:07,619] ERROR Error when closing producer (kafka.perf.ProducerPerformance$)org.apache.kafka.common.KafkaException: Error unregistering mbean at org.apache.kafka.common.metrics.JmxReporter.unregister(JmxReporter.java:100) at org.apache.kafka.common.metrics.JmxReporter.close(JmxReporter.java:90) at org.apache.kafka.common.metrics.Metrics.close(Metrics.java:204) at org.apache.kafka.clients.producer.KafkaProducer.close(KafkaProducer.java:279) at kafka.perf.ProducerPerformance$NewShinyProducer.close(ProducerPerformance.scala:222) at kafka.perf.ProducerPerformance$ProducerThread.run(ProducerPerformance.scala:299) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662)Caused by: javax.management.InstanceNotFoundException: kafka:type=producer at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094) at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:415) at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:403) at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:506) at org.apache.kafka.common.metrics.JmxReporter.unregister(JmxReporter.java:98) ... 8 more'\n"," b'The Kafka Consumer does not clean up all metrics after shutdown. It seems like this was a regression introduced in Kafka 2.4 when we added the KafkaConsumerMetrics class.'\n"," b'When scheduling an append, we currently only wakeup the IO thread after the batch is ready to drain. If the IO thread is blocking in `poll()`, there is no guarantee that it will get woken up by a subsequent append. We need to ensure that the thread gets woken up at least once when the linger timer starts ticking so that the IO thread will be ready when the batch is ready to drain.'\n"," b'Our current implementation ordering of {{KafkaConsumer.unsubscribe}} is the following: {code} this.subscriptions.unsubscribe(); this.coordinator.onLeavePrepare(); this.coordinator.maybeLeaveGroup(\"the consumer unsubscribed from all topics\"); {code} And inside {{onLeavePrepare}} we would look into the assignment and try to revoke them and notify users via {{RebalanceListener#onPartitionsRevoked}}, and then clear the assignment. However, the subscription\\'s assignment is already cleared in {{this.subscriptions.unsubscribe();}} which means user\\'s rebalance listener would never be triggered. In other words, from consumer client\\'s pov nothing is owned after unsubscribe, but from the user caller\\'s pov the partitions are not revoked yet. For callers like Kafka Streams which rely on the rebalance listener to maintain their internal state, this leads to inconsistent state management and failure cases. Before KIP-429 this issue is hidden away since every time the consumer re-joins the group later, it would still revoke everything anyways regardless of the passed-in parameters of the rebalance listener; with KIP-429 this is easier to reproduce now. I think we can summarize our fix as: \\xe2\\x80\\xa2 Inside `unsubscribe`, first do `onLeavePrepare / maybeLeaveGroup` and then `subscription.unsubscribe`. This we we are guaranteed that the streams\\' tasks are all closed as revoked by then. \\xe2\\x80\\xa2 [Optimization] If the generation is reset due to fatal error from join / hb response etc, then we know that all partitions are lost, and we should not trigger `onPartitionRevoked`, but instead just `onPartitionsLost` inside `onLeavePrepare`.'\n"," b'The Kafka consumer should have metrics around the number of seconds spent in rebalance over the last minute as well as the number of rebalances started during the previous minute.The other important thing about these metrics is that they should only be updated once per minute for example the rebalance time should not increase from second to second during a rebalance.'\n"," b'Cf KIP-233'\n"," b\"In the DSL, Kafka Streams applies an optimization for non-materialized tables: when these are queried an upstream state store is accessed. To ensure that the correct value is returned from the lookup, all intermediate processors after the materialized store, and before the processor that triggers the lookup are re-applied (cf `KTableValueGetter`). For re-applying DSL operators like filter/mapValues that works fine. However, for transformValue(), the method is executed with the incorrect `RecordContext` (note that DSL operators like filter don't have access to the `RecordContext` and thus, are not subject to this bug). Instead of using the record context from the value that was received from the upstream state store (and that is re-processed), the transformer would see the context from the record that triggered the lookup. Thus, the information about timestamp, offset, partition, topic name, and headers is incorrect.\"\n"," b'Hi,We have a REST api to publish to a topic. Yesterday, we started noticing that the producer is not able to produce messages at a good rate and the CLOSE_WAITs of our producer REST app are very high. All the producer REST requests are hence timing out.When we took the thread dump and analysed it, we noticed that the threads are getting blocked on JmxReporter metricChange. Here is the attached stack trace.\"dw-70 - POST /queues/queue_1/messages\" #70 prio=5 os_prio=0 tid=0x00007f043c8bd000 nid=0x54cf waiting for monitor entry [0x00007f04363c7000] java.lang.Thread.State: BLOCKED (on object monitor) at org.apache.kafka.common.metrics.JmxReporter.metricChange(JmxReporter.java:76) - waiting to lock <0x00000005c1823860> (a java.lang.Object) at org.apache.kafka.common.metrics.Metrics.registerMetric(Metrics.java:182) - locked <0x00000007a5e526c8> (a org.apache.kafka.common.metrics.Metrics) at org.apache.kafka.common.metrics.Sensor.add(Sensor.java:165) - locked <0x00000007a5e526e8> (a org.apache.kafka.common.metrics.Sensor)When I looked at the code of metricChange method, it uses a synchronised block on an object resource and it seems that it is held by another.'\n"," b'Currently, MBean `kafka.network:type=Processor,name=IdlePercent,networkProcessor=*` and `afka.network:type=SocketServer,name=NetworkProcessorAvgIdlePercent` could be greater than 1. However, these two values represent a percentage which should not exceed 1.'\n"," b'The Log implementation uses a custom copy-on-write array and a binary search to search for log segments. This is overengineered. We can delete this code and replace it with an off-the-shelf ConcurrentSkipListMap. This code seems to work so this is not pressing, just a simplification.'\n"," b\"Currently in the Raft's leader implementation, we validate that follower fetch offsets increase monotonically. This protects the guarantees that Raft provides since a non-monotonic update means that the follower has lost committed data, which may or may not result in data loss. It depends whether the update also causes a non-monotonic update to the high watermark. If the fetch is from an observer, no harm done since observers do not affect the high watermark. If the fetch is from a voter and a majority of nodes (excluding the fetcher) have offsets larger than or equal to the high watermark, also no harm done. It's easy to check for these cases and log a warning instead of raising an error. The question then is what to do if we get a voter fetch which does cause the high watermark to regress? The problem is that there are some scenarios where data loss might be unavoidable. For example, a follower's disk might become corrupt and ultimately get replaced. Often the damage is already done by the time we get the Fetch request with the non-monotonic offset, so the stricter validation in fact just prevents recovery. It's worth noting also that the stricter validation by the leader cannot be relied on to detect data loss. It could be the case that a recovered voter restarts in the middle of an election. There is no general way that I'm aware of that lets us detect when a voter has lost previously committed data. With all of this mind, my conclusion is that it makes sense to loosen the validation in fetches. The leader can still ensure that its high watermark does not go backwards and we can still log a warning, but it should not prevent replicas from catching up after hard failures with disk state loss.\"\n"," b\"Currently exactly-once producer is coupled with individual input partitions. This is not a well scaled solution for client application such as Kafka Streams, and the root cause is that EOS producer doesn't understand the topic partition move throughout consumer group rebalance. By amending this semantic gap, we could achieve much better EOS scalability.\"\n"," b'See [KIP-226|https://cwiki.apache.org/confluence/display/KAFKA/KIP-226+-+Dynamic+Broker+Configuration] for details. This will include the base implementation to enable dynamic updates of broker configs. SSL keystore update will be implemented as part of this task to enable testing.'\n"," b'Hi Kafka Team,We are running multiple webapps in tomcat container, and we have producer which are managed by the ServletContextListener (Lifecycle). Upon contextInitialized we create and on contextDestroyed we call the producer.close() but underlying Metric Lib does not shutdown. So we have thread leak due to this issue. I had to call Metrics.defaultRegistry().shutdown() to resolve this issue. is this know issue ? I know the metric lib have JVM Shutdown hook, but it will not be invoke since the contain thread is un-deploying the web app and class loader goes way and leaking thread does not find the under lying Kafka class. Because of this tomcat, it not shutting down gracefully.Are you guys planing to un-register metrics when Producer close is called or shutdown Metrics pool for client.id ? Here is logs:SEVERE: The web application [ ] appears to have started a thread named [metrics-meter-tick-thread-1] but has failed to stop it. This is very likely to create a memory leak.SEVERE: The web application [] appears to have started a thread named [metrics-meter-tick-thread-2] but has failed to stop it. This is very likely to create a memory leak.Thanks,Bhavesh'\n"," b'We added this parameter in KAFKA-1461, it changes existing behavior and is configurable by users. We should document the new behavior and the parameter.'\n"," b'When use IPv6 start the broker. The server.log output this error:===========[2016-05-25 15:45:56,120] WARN Error processing kafka.server:type=FetcherStats,name=RequestsPerSec,clientId=console-consumer-25184,brokerHost=fe80::92e2:baff:fe92:62f,brokerPort=3392 (com.yammer.metrics.reporting.JmxReporter)javax.management.MalformedObjectNameException: Invalid character \\':\\' in value part of propertyat javax.management.ObjectName.construct(ObjectName.java:618)at javax.management.ObjectName.<init>(ObjectName.java:1382)at com.yammer.metrics.reporting.JmxReporter.onMetricAdded(JmxReporter.java:395)at com.yammer.metrics.core.MetricsRegistry.notifyMetricAdded(MetricsRegistry.java:516)at com.yammer.metrics.core.MetricsRegistry.getOrAdd(MetricsRegistry.java:491)at com.yammer.metrics.core.MetricsRegistry.newMeter(MetricsRegistry.java:240)at kafka.metrics.KafkaMetricsGroup$class.newMeter(KafkaMetricsGroup.scala:80)at kafka.server.FetcherStats.newMeter(AbstractFetcherThread.scala:264)at kafka.server.FetcherStats.<init>(AbstractFetcherThread.scala:269)at kafka.server.AbstractFetcherThread.<init>(AbstractFetcherThread.scala:55)at kafka.consumer.ConsumerFetcherThread.<init>(ConsumerFetcherThread.scala:38)..........==============================In the AbstractFetcherThread.scala line 264:class FetcherStats(metricId: ClientIdAndBroker) extends KafkaMetricsGroup { val tags = Map(\"clientId\" -> metricId.clientId, \"brokerHost\" -> metricId.brokerHost, \"brokerPort\" -> metricId.brokerPort.toString)When brokerHost is IPv6, the address has :, which can\\'t use as ObjectName.'\n"," b'The AdminClient should contact multiple nodes before timing out a call. Right now, we could use up our entire call timeout just waiting for one very slow node to respond. We probably need to decouple the call timeout from the NetworkClient request timeout.'\n"," b\"{code} def immutableListenerConfigs(kafkaConfig: KafkaConfig, prefix: String): Map[String, AnyRef] = { newConfig.originals.asScala.filter { case (key, _) => key.startsWith(prefix) && !DynamicSecurityConfigs.contains(key) } } {code} We don't actually compare new configs to origin configs so the suitable exception is not thrown.\"\n"," b'RocksDbStore.java throws a mix of exceptions like StreamsException and ProcessorStateException. That needs to be made consistent. Also the exceptions thrown are not documented in the KeyValueStore interface. All the stores (RocksDb, InMemory) need to have consistent exception handling.Today a store error is fatal and halts the stream thread that is processing the exceptions. We could explore alternatives, like haling part of the topology that passes through that faulty store, i.e., failing tasks, not the entire thread.'\n"," b'Outstanding work left to be done for https://cwiki.apache.org/confluence/display/KAFKA/KIP-99%3A+Add+Global+Tables+to+Kafka+Streams'\n"," b'Saw the following transient failure.kafka.api.ProducerFailureHandlingTest > testWrongBrokerList PASSEDkafka.api.ProducerFailureHandlingTest > testNoResponse FAILED kafka.common.KafkaException: Socket server failed to bind to localhost:49013: Address already in use. at kafka.network.Acceptor.openServerSocket(SocketServer.scala:195) at kafka.network.Acceptor.<init>(SocketServer.scala:141) at kafka.network.SocketServer.startup(SocketServer.scala:68) at kafka.server.KafkaServer.startup(KafkaServer.scala:84) at kafka.utils.TestUtils$.createServer(TestUtils.scala:120) at kafka.api.ProducerFailureHandlingTest.setUp(ProducerFailureHandlingTest.scala:80) Caused by: java.net.BindException: Address already in use at sun.nio.ch.Net.bind(Native Method) at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:124) at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59) at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:52) at kafka.network.Acceptor.openServerSocket(SocketServer.scala:191) ... 5 more'\n"," b'In an application that depends on the Kafka server artifacts with: {code:xml} <dependency> <groupId>org.apache.kafka</groupId> <artifactId>kafka_2.11</artifactId> <version>1.1.0-SNAPSHOT</version> </dependency> {code} and then running the server programmatically, the following error occurs: {noformat} [2017-11-23 01:01:45,029] INFO Shutting down producer (kafka.producer.Producer:63) [2017-11-23 01:01:45,051] INFO Closing all sync producers (kafka.producer.ProducerPool:63) [2017-11-23 01:01:45,052] INFO Producer shutdown completed in 23 ms (kafka.producer.Producer:63) [2017-11-23 01:01:45,052] INFO [KafkaServer id=1] shutting down (kafka.server.KafkaServer:63) [2017-11-23 01:01:45,057] ERROR [KafkaServer id=1] Fatal error during KafkaServer shutdown. (kafka.server.KafkaServer:161) java.lang.NoClassDefFoundError: org/slf4j/event/Level at kafka.utils.CoreUtils$.swallow$default$3(CoreUtils.scala:83) at kafka.server.KafkaServer.shutdown(KafkaServer.scala:520) ... Caused by: java.lang.ClassNotFoundException: org.slf4j.event.Level at java.net.URLClassLoader$1.run(URLClassLoader.java:359) at java.net.URLClassLoader$1.run(URLClassLoader.java:348) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:347) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:312) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ... 25 more {noformat} It appears that KAFKA-1044 and [this PR|https://github.com/apache/kafka/pull/3477] removed the use of Log4J from Core but [added use of|https://github.com/confluentinc/kafka/commit/ed8b0315a6c3705b2a163ce3ab4723234779264f#diff-52505b9374ea885e44bcb73cbc4714d6R34] the {{org.slf4j.event.Level}} in {{CoreUtils.scala}}. The {{org.slf4j.event.Level}} class is in the {{org.slf4j:slf4j-api}} artifact, which is currently not included in the dependencies of {{org.apache.kafka:kafka_2.11:1.1.0-SNAPSHOT}}. Because this is needed by the server, the SLF4J API library probably needs to be added to the dependencies. [~viktorsomogyi] and [~ijuma], was this intentional, or is it intended that the SLF4J API be marked as {{provided}}? BTW, I marked this as CRITICAL just because this probably needs to be sorted out before the release. *Update*: As the comments below explain, the actual problem is a bit different to what is in the JIRA description.'\n"," b'See [KIP-226|https://cwiki.apache.org/confluence/display/KAFKA/KIP-226+-+Dynamic+Broker+Configuration] for details.'\n"," b'Configurations of connectors can get quite verbose when you have to specify the full class name, e.g. connector.class=org.apache.kafka.copycat.file.FileStreamSinkConnectorIt would be nice to allow connector classes to provide shorter aliases, e.g. something like \"file-sink\", to make this config less verbose. Flume does this, so we can use it as an example.'\n"," b'The word \"partition\" is referred to as \"partion\" in system_test/replication_testsuite/testcase_0106/testcase_0106_properties.json line 2 and core/src/main/scala/kafka/server/AbstractFetcherManager.scala line 49. This typo may interfere with text-based searching of output.'\n"," b'Our topic config cleanup policy seems to be broken. When a broker isbounced and starting up:1 - Read all the children of the config change path2 - For each, if the change id is greater than the last executed change, then extract the topic information.3 - If there is a log for that topic on this broker, then apply the change. However, if there is no log, then delete the config change.In step 3, a delete triggers a child change watch firing on all the otherbrokers. The other brokers currently take all the children of the configpath but will ignore those config changes that are less than the lastexecuted change. At least one issue here is that if a broker does not havepartitions for a topic then the lastExecutedChange is not updated (forthat topic).Consider this scenario:- Three brokers 0, 1, 2- Topic A has partitions only assigned to broker 0- Topic B has partitions only assigned to broker 1- Topic C has partitions only assigned to broker 2- Change 0: topic A- Change 1: topic B- Change 2: topic C- lastExecutedChange on broker 0 is 0- lastExecutedChange on broker 1 is 1- lastExecutedChange on broker 2 is 2- Bounce broker 1- The above bounce will cause Change 0 and Change 2 to get deleted.- Watch fires on broker 0 and 1- Broker 0 will try and read the topic corresponding to change 1 (since its lastExecutedChange is 0) and then change 2. That read will fail:2014/04/15 19:35:34.236 INFO [TopicConfigManager] [main] [kafka-server] [] Processed topic config change 25 for topic xyz, setting new config to {retention.ms=3600000, segment.ms=3600000}.2014/04/15 19:35:34.238 FATAL [KafkaServerStartable] [main] [kafka-server] [] Fatal error during KafkaServerStable startup. Prepare to shutdownorg.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /config/changes/config_change_0000000026 at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47) at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685) at org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:766) at org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:761) at kafka.utils.ZkUtils$.readData(ZkUtils.scala:467) at kafka.server.TopicConfigManager$$anonfun$kafka$server$TopicConfigManager$$processConfigChanges$2.apply(TopicConfigManager.scala:97) at kafka.server.TopicConfigManager$$anonfun$kafka$server$TopicConfigManager$$processConfigChanges$2.apply(TopicConfigManager.scala:93) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43) at kafka.server.TopicConfigManager.kafka$server$TopicConfigManager$$processConfigChanges(TopicConfigManager.scala:93) at kafka.server.TopicConfigManager.processAllConfigChanges(TopicConfigManager.scala:81) at kafka.server.TopicConfigManager.startup(TopicConfigManager.scala:72) at kafka.server.KafkaServer.startup(KafkaServer.scala:104) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34) ...Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /config/changes/config_change_0000000026 at org.apache.zookeeper.KeeperException.create(KeeperException.java:102) at org.apache.zookeeper.KeeperException.create(KeeperException.java:42) at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:927) at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:956) at org.I0Itec.zkclient.ZkConnection.readData(ZkConnection.java:103) at org.I0Itec.zkclient.ZkClient$9.call(ZkClient.java:770) at org.I0Itec.zkclient.ZkClient$9.call(ZkClient.java:766) at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675) ... 39 moreAnother issue is that there are two logging statements with incorrectqualifiers which makes things a little harder to debug. E.g.,2014/04/15 19:35:34.223 ERROR [TopicConfigManager] [kafka-server] [] Ignoring topic config change %d for topic %s since the change has expired'], shape=(32,), dtype=string)\n","tf.Tensor(\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]], shape=(32, 36), dtype=float32)\n"]}],"source":["# some train dataset examples\n","for input, target in train_dataset.take(2):\n","    print(input)\n","    print(target)"]},{"cell_type":"markdown","source":["BERT MODEL"],"metadata":{"id":"iw8d8KX_oGlg"}},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bw0KuVLrsl1t","executionInfo":{"status":"ok","timestamp":1642516801250,"user_tz":-60,"elapsed":9400,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"d3f3652b-a02d-4255-c767-ed6c6cfc9495"},"outputs":[{"output_type":"stream","name":"stdout","text":["BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"]}],"source":["bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2' \n","\n","map_name_to_handle = {\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'}\n","\n","map_model_to_preprocess = {\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'}\n","\n","# find more models and preprocessing maps at \n","# https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n","\n","\n","bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n","bert_model = hub.KerasLayer(tfhub_handle_encoder)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kXN1A3s2sl1u","executionInfo":{"status":"ok","timestamp":1642516819725,"user_tz":-60,"elapsed":341,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"cabfb20b-307e-4325-c725-d15185204665"},"outputs":[{"output_type":"stream","name":"stdout","text":["Lines      : [b'This is an umbrella story for capturing changes to Kafka Streams to enable Queryable state as described in KIP-67 https://cwiki.apache.org/confluence/display/KAFKA/KIP-67%3A+Queryable+state+for+Kafka+Streams.', b'Pretty much as soon as we started actively monitoring the _last-rebalance-seconds-ago_ metric in our Kafka Streams test environment, we started seeing something weird. Every so often one of the StreamThreads (ie a single Consumer instance) would appear to permanently fall out of the group, as evidenced by a monotonically increasing _last-rebalance-seconds-ago._ We inject artificial network failures every few hours at most, so the group rebalances quite often. But the one consumer never rejoins, with no other symptoms (besides a slight drop in throughput since the remaining threads had to take over this member\\'s work). We\\'re confident that the problem exists in the client layer, since the logs confirmed that the unhealthy consumer was still calling poll. It was also calling Consumer#committed in its main poll loop, which was consistently failing with a TimeoutException. When I attached a remote debugger to an instance experiencing this issue, the network client\\'s connection to the group coordinator (the one that uses MAX_VALUE - node.id as the coordinator id) was in the DISCONNECTED state. But for some reason it never tried to re-establish this connection, although it did successfully connect to that same broker through the \"normal\" connection (ie the one that juts uses node.id). The tl;dr is that the AbstractCoordinator\\'s FindCoordinatorRequest has failed (presumably due to a disconnect), but the _findCoordinatorFuture_ is non-null so a new request is never sent. This shouldn\\'t be possible since the FindCoordinatorResponseHandler is supposed to clear the _findCoordinatorFuture_ when the future is completed. But somehow that didn\\'t happen, so the consumer continues to assume there\\'s still a FindCoordinator request in flight and never even notices that it\\'s dropped out of the group. These are the only confirmed findings so far, however we have some guesses which I\\'ll leave in the comments. Note that we only noticed this due to the newly added _last-rebalance-seconds-ago_ __metric, and there\\'s no reason to believe this bug hasn\\'t been flying under the radar since the Consumer\\'s inception']\n","Keys       : ['input_mask', 'input_word_ids', 'input_type_ids']\n","Shape      : (2, 128)\n","Word Ids   :\n","[[  101  2023  2003  2019 12977  2466  2005 11847  3431  2000 10556 24316]\n"," [  101  3492  2172  2004  2574  2004  2057  2318  8851  8822  1996  1035]]\n","Input Mask :\n","[[1 1 1 1 1 1 1 1 1 1 1 1]\n"," [1 1 1 1 1 1 1 1 1 1 1 1]]\n","Type Ids   :\n","[[0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0]]\n"]}],"source":["# preprocessing examples\n","text_test = [input[0].numpy(), input[1].numpy()]\n","text_preprocessed = bert_preprocess_model(text_test)\n","\n","print(f'Lines      : {text_test}')\n","print(f'Keys       : {list(text_preprocessed.keys())}')\n","print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n","print(f'Word Ids   :')\n","print(f'{text_preprocessed[\"input_word_ids\"][:, :12]}')\n","print(f'Input Mask :')\n","print(f'{text_preprocessed[\"input_mask\"][:, :12]}')\n","print(f'Type Ids   :')\n","print(f'{text_preprocessed[\"input_type_ids\"][:, :12]}')\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKfmYDGUsl1u","executionInfo":{"status":"ok","timestamp":1642516835589,"user_tz":-60,"elapsed":2138,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"6aa5175b-2c6b-4557-bda5-b53b0d686c66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n","Sequence Outputs Shape:(2, 128, 128)\n","Sequence Outputs Values:[[-0.8199718  -0.06247935 -3.4160364  ... -1.6043428  -1.8216697\n","   1.4438787 ]\n"," [-1.8183875   1.0086884   0.603252   ... -2.5280254  -1.550652\n","   0.4165143 ]\n"," [-1.3335456   0.9950316   0.23367187 ... -2.314034   -1.6054122\n","   0.39064446]\n"," ...\n"," [ 0.37140742  0.51852304 -0.26419178 ... -2.014215   -0.7632927\n","   1.3377297 ]\n"," [-0.19957796  0.33347338 -0.07792255 ... -1.583926   -1.9047812\n","   2.841353  ]\n"," [ 0.8625738   0.7430699  -0.6469157  ... -1.4979379  -1.8239652\n","   2.6709402 ]]\n","Pooled Outputs Shape:(2, 128)\n","Pooled Outputs Values:[-0.9999571   0.02029373 -0.865917    0.5325085  -0.8904524   0.8826034\n"," -0.71303236 -0.995001   -0.02119065 -0.03614308 -0.69785786 -0.08544977]\n"]}],"source":["# model output examples\n","\n","bert_results = bert_model(text_preprocessed)\n","\n","print(f'Loaded BERT: {tfhub_handle_encoder}')\n","print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n","print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')\n","print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n","print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')"]},{"cell_type":"markdown","source":["Classifier"],"metadata":{"id":"9uFsfBpQorAy"}},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2021-12-01T12:17:42.681396Z","iopub.status.busy":"2021-12-01T12:17:42.680658Z","iopub.status.idle":"2021-12-01T12:17:42.682766Z","shell.execute_reply":"2021-12-01T12:17:42.683119Z"},"id":"aksj743St9ga","executionInfo":{"status":"ok","timestamp":1642516898070,"user_tz":-60,"elapsed":209,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}}},"outputs":[],"source":["def build_classifier_model():\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","  encoder_inputs = preprocessing_layer(text_input)\n","  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  net = outputs['pooled_output'] \n","  net = tf.keras.layers.Dropout(0.1)(net)\n","  net = tf.keras.layers.Dense(len(assignee_dict), activation=None, name='classifier')(net)\n","  return tf.keras.Model(text_input, net)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BdE9g3F2sl1w","executionInfo":{"status":"ok","timestamp":1642516917958,"user_tz":-60,"elapsed":9004,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"bb031b16-a87d-4ffc-ef41-943372f565b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," text (InputLayer)              [(None,)]            0           []                               \n","                                                                                                  \n"," preprocessing (KerasLayer)     {'input_type_ids':   0           ['text[0][0]']                   \n","                                (None, 128),                                                      \n","                                 'input_word_ids':                                                \n","                                (None, 128),                                                      \n","                                 'input_mask': (Non                                               \n","                                e, 128)}                                                          \n","                                                                                                  \n"," BERT_encoder (KerasLayer)      {'pooled_output': (  4385921     ['preprocessing[0][0]',          \n","                                None, 128),                       'preprocessing[0][1]',          \n","                                 'default': (None,                'preprocessing[0][2]']          \n","                                128),                                                             \n","                                 'encoder_outputs':                                               \n","                                 [(None, 128, 128),                                               \n","                                 (None, 128, 128)],                                               \n","                                 'sequence_output':                                               \n","                                 (None, 128, 128)}                                                \n","                                                                                                  \n"," dropout (Dropout)              (None, 128)          0           ['BERT_encoder[0][3]']           \n","                                                                                                  \n"," classifier (Dense)             (None, 36)           4644        ['dropout[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 4,390,565\n","Trainable params: 4,390,564\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n"]}],"source":["# building the classifier\n","\n","bug_assignee_classifier = build_classifier_model()\n","\n","bug_assignee_classifier.summary()"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xWUeyMX1vEs","outputId":"fe958bb0-b06c-4a08-dc73-cb0d9a7da40b","executionInfo":{"status":"ok","timestamp":1642516924775,"user_tz":-60,"elapsed":562,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[0.02376875 0.06116279 0.02803982 0.01546968 0.05811377 0.00842309\n","  0.00687862 0.02491615 0.02883748 0.1261643  0.0528581  0.01137536\n","  0.01097933 0.03139823 0.02767351 0.0143291  0.00196825 0.00763957\n","  0.06159985 0.00417969 0.051322   0.06009429 0.00891856 0.01083429\n","  0.01087304 0.05311107 0.00862431 0.0048587  0.00759663 0.01343136\n","  0.00751366 0.01122719 0.02806221 0.10479265 0.00729506 0.00566956]\n"," [0.01462017 0.04762396 0.01645673 0.02841918 0.08085826 0.0049979\n","  0.01313331 0.01415737 0.03057132 0.12987724 0.04235459 0.02769663\n","  0.01560683 0.02988086 0.02503457 0.02222097 0.00191087 0.01137038\n","  0.05191648 0.0059737  0.07414009 0.06073701 0.00482728 0.00651438\n","  0.00542167 0.04406381 0.01299875 0.00302255 0.01164023 0.01937752\n","  0.00497521 0.01213653 0.02317544 0.08592618 0.01228298 0.00407906]], shape=(2, 36), dtype=float32)\n"]}],"source":["# classifier output examples\n","\n","bert_raw_result = bug_assignee_classifier(tf.constant(text_test))\n","print(tf.nn.softmax(bert_raw_result))"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"id":"omLYS3x3sl1x","executionInfo":{"status":"ok","timestamp":1642516936033,"user_tz":-60,"elapsed":460,"user":{"displayName":"ayberk tecimer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaP-YEMaVtdZNLQ7v0pH6E9_gs4QaOtrqxEDk5w=s64","userId":"07258608254476078813"}},"outputId":"b1b165c7-3673-43f0-9ac8-6f1b45b3c802"},"outputs":[{"output_type":"execute_result","data":{"image/png":"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\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":24}],"source":["# plotting the model (requires pydot and graphviz)\n","tf.keras.utils.plot_model(bug_assignee_classifier)"]},{"cell_type":"code","source":[""],"metadata":{"id":"hevr3hj-o3JE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwlUDl6asl1x"},"outputs":[],"source":["epochs = 15\n","steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n","num_train_steps = steps_per_epoch * epochs\n","\n","# Here we are using AdamW with a linear learning rate schedule.\n","# Because of the rolling averages in the optimization logic we need a few warm up steps.\n","warmup_ratio = 0.1\n","num_warmup_steps = int(warmup_ratio*num_train_steps)\n","init_lr = 1e-3\n","\n","optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')\n","\n","loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","metrics = tf.keras.metrics.CategoricalAccuracy()\n","\n","# compiling the model\n","bug_assignee_classifier.compile(optimizer=optimizer,\n","                         loss=loss,\n","                         metrics=metrics)\n"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"name":"BugAssigneeRecommendation.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}