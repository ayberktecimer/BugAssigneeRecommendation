{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m venv demo-env\n",
    "# source demo-env/bin/activate\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from keras import regularizers\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading json file that includes fake names and corresponding id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_assignee_dict(path):\n",
    "    with open(path, 'r') as f:\n",
    "        assignee_dict = json.load(f)\n",
    "    return assignee_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignee_dict = read_assignee_dict('datasets/assignee_dict.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model paths that are trained in the training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model_path = 'demo-models/bow_model_3'\n",
    "rnn_model_path = 'demo-models/rnn_model_3'\n",
    "bert_model_path = 'demo-models/bert_model_4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading trained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "bow_model_reloaded = tf.keras.models.load_model(bow_model_path)\n",
    "rnn_model_reloaded = tf.keras.models.load_model(rnn_model_path)\n",
    "bert_model_reloaded = tf.keras.models.load_model(bert_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading saved input vectorizer for Bag of words model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vectorizer_data = pickle.load(open(\"demo-models/input_vectorizer_1.pkl\", \"rb\"))\n",
    "input_vectorizer_loaded = layers.TextVectorization.from_config(input_vectorizer_data['config'])\n",
    "# You have to call `adapt` with some dummy data (BUG in Keras) https://stackoverflow.com/questions/65103526/how-to-save-textvectorization-to-disk-in-tensorflow\n",
    "input_vectorizer_loaded.adapt(tf.data.Dataset.from_tensor_slices([\"\"]))\n",
    "input_vectorizer_loaded.set_weights(input_vectorizer_data['weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to recompile BERT model with optimizer, loss, and metrics as adamw is not available for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-3\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=2070,\n",
    "                                          num_warmup_steps=207,\n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "# compiling the model\n",
    "bert_model_reloaded.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pipeline that takes test data (one issue) model and model type. According to inputs it returns prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_pipeline(test_data,model,model_type):\n",
    "    if model_type == 'bow':\n",
    "        test_data = input_vectorizer_loaded(test_data)\n",
    "        #print(test_data)\n",
    "        test_data = tf.reshape(test_data, [1,13897])\n",
    "        prediction = model.predict(test_data)\n",
    "        prediction = prediction.flatten()\n",
    "        #print(prediction)\n",
    "    elif model_type == 'rnn':\n",
    "        test_data = tf.constant([test_data])\n",
    "        prediction = model.predict(test_data)\n",
    "        prediction = prediction.flatten()\n",
    "        #print(prediction)\n",
    "    elif model_type == 'bert':\n",
    "        test_data = tf.constant([test_data])\n",
    "        prediction = model.predict(test_data)\n",
    "        prediction = prediction.flatten()\n",
    "        #print(prediction)\n",
    "    return prediction\n",
    "\n",
    "def calculate_top_n_prediction(prediction, n, assignee_dict):\n",
    "    print('Top {} predictions:'.format(n))\n",
    "    top_predictions = (-prediction).argsort()[:n]\n",
    "    for pred in top_predictions:\n",
    "        print(list(assignee_dict.keys())[list(assignee_dict.values()).index(pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo data that was in the test dataset during training ground truth is **Jennifer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = 'We need to add the new requests response for the new consumer https cwiki apache org confluence display KAFKA Kafka Consumer Rewrite Design using the protocol definition'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "Alice\n",
      "Bob\n",
      "Charlie\n",
      "Jennifer\n",
      "Patricia\n"
     ]
    }
   ],
   "source": [
    "bow_prediction = prediction_pipeline(test_data,model=bow_model_reloaded,model_type='bow')\n",
    "calculate_top_n_prediction(bow_prediction, n=5, assignee_dict=assignee_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "Jennifer\n",
      "Thomas\n",
      "Mary\n",
      "Richard\n",
      "Jan\n"
     ]
    }
   ],
   "source": [
    "rnn_prediction = prediction_pipeline(test_data,model=rnn_model_reloaded,model_type='rnn')\n",
    "calculate_top_n_prediction(rnn_prediction, n=5, assignee_dict=assignee_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "Jennifer\n",
      "Sophia\n",
      "Charlie\n",
      "Julia\n",
      "Tom\n"
     ]
    }
   ],
   "source": [
    "bert_prediction = prediction_pipeline(test_data,model=bert_model_reloaded,model_type='bert')\n",
    "calculate_top_n_prediction(bert_prediction, n=5, assignee_dict=assignee_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Demo Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo data that was in the test dataset during training ground truth is **Steven**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"In KIP we are reusing the request timeout ms to timeout the batches in the accumulator We were intended to avoid the case that the batches sitting in the accumulator forever when topic metadata is missing Currently we are not checking if metadata is available or not when we timeout the batches in the accumulator although the comments says we will check the metadata This causes problem that once the previous batch hit a request timeout and got retried all the subsequent batches will fail with timeout exception We should only timeout the batches in the accumulator when the metadata of the partition is missing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "Alice\n",
      "Charlie\n",
      "Natalie\n",
      "Vanessa\n",
      "Bob\n"
     ]
    }
   ],
   "source": [
    "bow_prediction = prediction_pipeline(test_data,model=bow_model_reloaded,model_type='bow')\n",
    "calculate_top_n_prediction(bow_prediction, n=5, assignee_dict=assignee_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "Steven\n",
      "Vanessa\n",
      "Mary\n",
      "Julia\n",
      "Alice\n"
     ]
    }
   ],
   "source": [
    "rnn_prediction = prediction_pipeline(test_data,model=rnn_model_reloaded,model_type='rnn')\n",
    "calculate_top_n_prediction(rnn_prediction, n=5, assignee_dict=assignee_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "Steven\n",
      "Thomas\n",
      "Sophia\n",
      "Anna\n",
      "Melissa\n"
     ]
    }
   ],
   "source": [
    "bert_prediction = prediction_pipeline(test_data,model=bert_model_reloaded,model_type='bert')\n",
    "calculate_top_n_prediction(bert_prediction, n=5, assignee_dict=assignee_dict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df55a2a536a21f30b06128125f4338b263134174fec40b5b1c1d9a50df01c28b"
  },
  "kernelspec": {
   "display_name": "seminar_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
